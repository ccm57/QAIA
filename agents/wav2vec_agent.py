#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Module de reconnaissance vocale utilisant Wav2Vec2.

"""

# /// script
# dependencies = [
#   "torch>=2.0.0",
#   "sounddevice>=0.4.5",
#   "scipy>=1.9.0", # Pour scipy.io.wavfile
#   "transformers>=4.26.0",
#   "numpy>=1.22.0"
# ]
# ///

import os
import logging
import traceback
from pathlib import Path
import threading
import time
from typing import Optional, Tuple, Dict, List
import torch
import gc
import numpy as np
import sounddevice as sd
import scipy.io.wavfile as wav
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from utils.monitoring import record_timing

try:
    from config.system_config import MODEL_CONFIG
except ImportError:
    MODEL_CONFIG = None

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
logger.propagate = True

def record_audio(duration: float = 5.0, sample_rate: int = 16000, output_dir: str = None) -> str:
    """
    Enregistre un court extrait audio via une fonction de module (compatibilit√©).

    Args:
        duration (float): Dur√©e en secondes.
        sample_rate (int): Fr√©quence d'√©chantillonnage.
        output_dir (str): Dossier de sortie. Par d√©faut, data/audio.

    Returns:
        str: Chemin du fichier WAV cr√©√©, None en cas d'erreur.
    """
    try:
        base_dir = Path(__file__).parent.parent.absolute()
        audio_dir = Path(output_dir) if output_dir else (base_dir / "data" / "audio")
        audio_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Enregistrement audio module-level: {duration}s @ {sample_rate}Hz")
        frames = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')
        sd.wait()

        timestamp = int(time.time())
        audio_file = audio_dir / f"recording_{timestamp}.wav"
        wav.write(str(audio_file), sample_rate, (frames * 32767).astype(np.int16))
        return str(audio_file) if audio_file.exists() else None
    except Exception as e:
        logger.error(f"Erreur record_audio (module): {e}")
        return None


def transcribe_audio(audio_path: str) -> str:
    """
    Fonction de compatibilit√© module-level: transcrit un fichier audio.

    Args:
        audio_path (str): Chemin du fichier audio.

    Returns:
        str: Transcription ou message d'erreur.
    """
    try:
        agent = Wav2VecVoiceAgent()
        text, _conf = agent.transcribe_audio(audio_path)
        return text
    except Exception as e:
        logger.error(f"Erreur transcribe_audio (module): {e}")
        return f"Erreur: {e}"

class Wav2VecVoiceAgent:
    """Agent de reconnaissance vocale simplifi√© pour QAIA."""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Impl√©mentation thread-safe du singleton."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self, config_path="CONFIG.py", debug=False, enable_monitoring=True, preferred_model: str = "jonatasgrosman/wav2vec2-large-xlsr-53-french"):
        """
        Initialise l'agent de reconnaissance vocale.
        
        Args:
            config_path (str): Chemin vers le fichier de configuration
            debug (bool): Active le mode debug avec plus de logs
            enable_monitoring (bool): Active le monitoring des performances
        """
        if self._initialized:
            return
            
        self.logger = logging.getLogger(__name__)
        
        # Configurer le niveau de log
        self.debug = debug
        if debug:
            self.logger.setLevel(logging.DEBUG)
        
        self.logger.info("Initialisation de l'agent vocal (version corrig√©e)")
        
        # Chemins
        self.base_dir = Path(__file__).parent.parent.absolute()
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # CONFIGURATION STT (centralis√©e via system_config)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if MODEL_CONFIG and "speech" in MODEL_CONFIG:
            speech_cfg = MODEL_CONFIG["speech"]
            self.sample_rate = int(speech_cfg.get("sampling_rate", 16000))
            use_gpu_stt = False
            if MODEL_CONFIG.get("gpu_audio") and MODEL_CONFIG["gpu_audio"].get("USE_GPU_FOR_STT"):
                use_gpu_stt = torch.cuda.is_available()
            self.device = "cuda" if use_gpu_stt else str(speech_cfg.get("device", "cpu"))
        else:
            self.sample_rate = 16000
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Cache HuggingFace : s'assurer que le r√©pertoire existe et est accessible
        self.hf_cache_dir = self.base_dir / "models" / "huggingface_cache"
        self.hf_cache_dir.mkdir(parents=True, exist_ok=True)
        os.environ['HF_HOME'] = str(self.hf_cache_dir)
        os.environ['HUGGINGFACE_HUB_CACHE'] = str(self.hf_cache_dir)
        self.audio_dir = self.base_dir / "data" / "audio"
        self.audio_dir.mkdir(parents=True, exist_ok=True)
        
        # √âtat
        self._model_loaded = False
        self._conversation_mode = False
        self._lazy_load = True
        self._last_load_error: Optional[str] = None
        self._load_lock = threading.Lock()
        
        # Mod√®le et processeur
        self.model = None
        self.processor = None
        self.preferred_model = preferred_model
        self.model_name = self.preferred_model  
        # Mod√®le de secours (base stable)
        self.fallback_model = "facebook/wav2vec2-base-960h"
        self.correction_dict = {}
        self.transcription_history = []
        self.max_history_size = 10
        self.AUDIO_DIR = self.audio_dir
        
        # Initialisation du monitoring simplifi√©
        self.enable_monitoring = enable_monitoring
        
        self._initialized = True
        self.logger.info(f"Agent vocal initialis√© (GPU: {self.device})")
        # D√©sactiver torch.compile/dynamo pour √©viter les tensors 'meta' avec chargement Flax
        try:
            import torch._dynamo as _dynamo  # type: ignore
            _dynamo.disable()
        except Exception:
            pass
        os.environ.setdefault("TORCH_COMPILE", "0")
    
    def _ensure_model_loaded(self, force_reload=False) -> bool:
        """
        S'assure que le mod√®le soit charg√© avant une inf√©rence.
        Thread-safe : un seul chargement √† la fois.
        
        Args:
            force_reload (bool): Force le rechargement m√™me si d√©j√† charg√©
            
        Returns:
            bool: True si le mod√®le est charg√© avec succ√®s, False sinon
        """
        if self._model_loaded and not force_reload:
            return True

        with self._load_lock:
            if self._model_loaded and not force_reload:
                return True
            self._last_load_error = None
            try:
                model_name = self.preferred_model
                self.logger.info(f"üîÑ Chargement mod√®le STT: {model_name} (cache: {self.hf_cache_dir})")

                try:
                    self.processor = Wav2Vec2Processor.from_pretrained(model_name)
                    self.model = Wav2Vec2ForCTC.from_pretrained(
                        model_name,
                        torch_dtype=torch.float32,
                    )
                    self.logger.info(f"‚úÖ Mod√®le STT charg√©: {model_name}")
                except Exception as e:
                    self.logger.error(f"‚ùå √âchec chargement {model_name}: {e}")
                    if self.fallback_model and self.fallback_model != model_name:
                        self.logger.warning(f"‚ö†Ô∏è Tentative fallback: {self.fallback_model}")
                        try:
                            self.processor = Wav2Vec2Processor.from_pretrained(self.fallback_model)
                            self.model = Wav2Vec2ForCTC.from_pretrained(
                                self.fallback_model,
                                torch_dtype=torch.float32,
                            )
                            model_name = self.fallback_model
                            self.model_name = self.fallback_model
                            self.logger.info(f"‚úÖ Fallback actif: {self.fallback_model}")
                        except Exception as e2:
                            self.logger.error(f"‚ùå Fallback √©chou√©: {e2}")
                            raise
                    else:
                        raise

                if self.device == "cuda":
                    self.model = self.model.to(self.device)
                self.model.eval()
                self._model_loaded = True
                return True

            except Exception as e:
                self._last_load_error = str(e)
                self.logger.error(f"Erreur lors du chargement du mod√®le STT: {e}")
                self.logger.error(traceback.format_exc())
                return False
            
    def _force_unload_model(self):
        """Force le d√©chargement du mod√®le et du processeur de la m√©moire."""
        try:
            self.model = None
            self.processor = None
            self._model_loaded = False
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("Mod√®le d√©charg√© de la m√©moire")
            return True
        except Exception as e:
            self.logger.error(f"Erreur lors du d√©chargement du mod√®le: {e}")
            return False
    
    def _preprocess_audio(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """
        Pr√©traite l'audio pour am√©liorer la qualit√© STT.
        Applique filtrage, normalisation et r√©duction bruit.
        
        Args:
            audio_data: Signal audio (numpy array float32)
            sample_rate: Fr√©quence d'√©chantillonnage
            
        Returns:
            Audio pr√©trait√©
        """
        from scipy.signal import butter, filtfilt
        
        # 1. Filtrage passe-haut (√©liminer bruit basse fr√©quence < 100Hz, plus agressif)
        nyquist = sample_rate / 2
        cutoff = 100  # Hz (augment√© de 80 √† 100 pour mieux √©liminer le bruit)
        b, a = butter(4, cutoff / nyquist, btype='high')
        audio_data = filtfilt(b, a, audio_data)
        
        # 2. Normalisation RMS cible (augment√©e pour meilleure intelligibilit√©)
        target_rms = 0.20  # Augment√© de 0.15 √† 0.20 pour signal plus fort
        current_rms = np.sqrt(np.mean(audio_data**2))
        if current_rms > 1e-6:
            gain = target_rms / current_rms
            # Limiter le gain pour √©viter amplification excessive du bruit
            gain = np.clip(gain, 0.5, 4.0)  # Augment√© de 3.0 √† 4.0 pour permettre plus de gain
            audio_data = audio_data * gain
            self.logger.debug(f"Normalisation RMS: {current_rms:.3f} ‚Üí {target_rms:.3f} (gain={gain:.2f})")
        
        # 3. Clipping soft (√©viter distorsion dure)
        # Utilise tanh pour compression douce des pics
        audio_data = np.tanh(audio_data * 1.2) / 1.2
        
        # 4. R√©duction de bruit simple (filtre m√©dian pour √©liminer les pics isol√©s)
        # Appliquer un filtre m√©dian sur de tr√®s courtes fen√™tres pour √©liminer les clics
        if len(audio_data) > 10:
            from scipy.signal import medfilt
            audio_data = medfilt(audio_data, kernel_size=3)
        
        # 5. Normalisation finale pour √©viter saturation
        max_val = np.abs(audio_data).max()
        if max_val > 0.95:  # Si proche de la saturation
            audio_data = audio_data * 0.95 / max_val
        
        return audio_data

    def transcribe_audio(self, audio_path: str, force_reload: bool = False) -> Tuple[str, float]:
        """
        Transcrit un fichier audio en texte.
        
        Args:
            audio_path (str): Chemin vers le fichier audio √† transcrire
            force_reload (bool): Force le rechargement du mod√®le
            
        Returns:
            tuple: (texte transcrit, score de confiance)
        """
        try:
            # Mesurer le temps de transcription
            start_time = time.time()
            t_checkpoint = start_time
            
            # Charger le mod√®le si n√©cessaire
            if not self._ensure_model_loaded(force_reload):
                err = getattr(self, "_last_load_error", None) or ""
                hint = (" " + err.replace("\n", " ")[:80] + ("‚Ä¶" if len(err) > 80 else "")) if err else ""
                return f"Erreur: Mod√®le non disponible{hint}", 0.0
            else:
                record_timing("asr", "load_model", time.time() - t_checkpoint)
                t_checkpoint = time.time()
            
            # V√©rifier le fichier
            if not audio_path or not os.path.isfile(audio_path):
                self.logger.error(f"Fichier audio introuvable: {audio_path}")
                return "Erreur: fichier audio introuvable", 0.0

            self.logger.info(f"Transcription de: {audio_path}")
            
            # Charger l'audio
            sample_rate, audio_data = wav.read(audio_path)
            record_timing("asr", "read_wav", time.time() - t_checkpoint)
            t_checkpoint = time.time()
            
            # Assurer mono
            if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:
                # Moyenne des canaux ‚Üí mono
                audio_data = audio_data.mean(axis=1)

            # Conversion en float32 si n√©cessaire
            if audio_data.dtype == np.int16:
                audio_data = audio_data.astype(np.float32) / 32768.0
            elif audio_data.dtype == np.int32:
                audio_data = (audio_data.astype(np.float32) / 2147483648.0)
            elif audio_data.dtype == np.float64:
                audio_data = audio_data.astype(np.float32)
            
            # R√©√©chantillonner si n√©cessaire
            if sample_rate != self.sample_rate:
                from scipy.signal import resample_poly
                # Utiliser un r√©√©chantillonnage polyphas√© plus rapide et pr√©cis
                audio_data = resample_poly(audio_data, self.sample_rate, sample_rate)
            record_timing("asr", "resample", time.time() - t_checkpoint)
            t_checkpoint = time.time()
            
            # NOUVEAU: Pr√©traitement audio pour am√©liorer qualit√© STT
            audio_data = self._preprocess_audio(audio_data, self.sample_rate)
            record_timing("asr", "preprocess_audio", time.time() - t_checkpoint)
            t_checkpoint = time.time()
            
            # Pr√©processer avec le processor
            inputs = self.processor(
                audio_data,
                sampling_rate=self.sample_rate,
                return_tensors="pt",
                padding=True
            )
            record_timing("asr", "preprocess", time.time() - t_checkpoint)
            t_checkpoint = time.time()
            # Forcer CPU/float32 pour √©viter 'meta' device issues
            inputs = {k: v.to("cpu", dtype=torch.float32) for k, v in inputs.items()}
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Inf√©rence
            with torch.no_grad():
                logits = self.model(**inputs).logits
            record_timing("asr", "inference", time.time() - t_checkpoint)
            t_checkpoint = time.time()
            
            # D√©coder avec CTC
            predicted_ids = torch.argmax(logits, dim=-1)
            
            # CORRECTION: Utiliser decode au lieu de batch_decode pour CTC
            # batch_decode ne g√®re pas correctement les tokens CTC r√©p√©t√©s
            transcription = self.processor.decode(predicted_ids[0])
            
            record_timing("asr", "decode", time.time() - t_checkpoint)
            
            # Calculer un score de confiance simple
            confidence = float(torch.max(torch.softmax(logits, dim=-1)).cpu())
            
            # Calculer le temps de transcription
            transcription_time = time.time() - start_time
            record_timing("asr", "transcription", transcription_time)
            
            # Ajouter √† l'historique
            self.transcription_history.append((transcription, confidence))
            if len(self.transcription_history) > self.max_history_size:
                self.transcription_history.pop(0)
            
            # Log de statistiques
            self.logger.info(f"Transcription effectu√©e en {transcription_time:.2f}s avec confiance {confidence:.2f}")
            
            return transcription, confidence
            
        except Exception as e:
            self.logger.error(f"Erreur de transcription: {e}")
            self.logger.error(traceback.format_exc())
            return f"Erreur: {str(e)}", 0.0

    def transcribe_with_events(self, audio_path: str, force_reload: bool = False) -> Tuple[str, float]:
        """
        Transcrit un fichier audio en texte avec √©mission d'√©v√©nements temps r√©el.
        
        Args:
            audio_path (str): Chemin vers le fichier audio √† transcrire
            force_reload (bool): Force le rechargement du mod√®le
            
        Returns:
            tuple: (texte transcrit, score de confiance)
        """
        from interface.events.event_bus import event_bus
        
        try:
            # √âmettre d√©but transcription
            # √âmettre √©v√©nement agent.state_change pour STT (EN_COURS)
            event_data_start = {
                'name': 'STT',
                'status': 'EN_COURS',
                'activity_percentage': 50.0,
                'details': 'Transcription audio en cours...',
                'last_update': time.time()
            }
            event_bus.emit('agent.state_change', event_data_start)
            self.logger.info(f"√âv√©nement agent.state_change √©mis pour STT (EN_COURS): {event_data_start}")
            
            event_bus.emit('stt.start', {
                'timestamp': time.time(),
                'audio_path': audio_path
            })
            
            self.logger.info(f"Transcription avec √©v√©nements: {audio_path}")
            
            # √âmettre progression
            event_bus.emit('stt.transcribing', {
                'timestamp': time.time(),
                'status': 'Chargement du mod√®le...'
            })
            
            # Assurer mod√®le charg√©
            if not self._ensure_model_loaded(force_reload):
                err = getattr(self, "_last_load_error", None) or ""
                hint = (" " + err.replace("\n", " ")[:80] + ("‚Ä¶" if len(err) > 80 else "")) if err else ""
                error_msg = f"Erreur: Mod√®le non disponible{hint}"
                event_bus.emit('stt.error', {
                    'timestamp': time.time(),
                    'error': error_msg
                })
                return error_msg, 0.0
            
            # √âmettre progression
            event_bus.emit('stt.transcribing', {
                'timestamp': time.time(),
                'status': 'Analyse audio...'
            })
            
            # Effectuer transcription (r√©utilise la logique existante)
            transcription, confidence = self.transcribe_audio(audio_path, force_reload=False)
            
            # √âmettre compl√©tion
            # √âmettre √©v√©nement agent.state_change pour STT (ACTIF apr√®s transcription)
            # IMPORTANT: S'assurer que l'√©v√©nement est bien √©mis m√™me si transcription contient "Erreur"
            if transcription and not transcription.lower().startswith("erreur"):
                event_bus.emit('agent.state_change', {
                    'name': 'STT',
                    'status': 'ACTIF',
                    'activity_percentage': 100.0,
                    'details': f'Transcription termin√©e: "{transcription[:50] if transcription else "N/A"}..."',
                    'last_update': time.time()
                })
            else:
                # En cas d'erreur, √©mettre statut ERREUR
                event_bus.emit('agent.state_change', {
                    'name': 'STT',
                    'status': 'ERREUR',
                    'activity_percentage': 0.0,
                    'details': f'Erreur transcription: {transcription}',
                    'last_update': time.time()
                })
            
            event_bus.emit('stt.complete', {
                'timestamp': time.time(),
                'transcription': transcription,
                'confidence': confidence
            })
            
            # Log pour debug
            self.logger.info(f"√âv√©nement agent.state_change √©mis pour STT: status=ACTIF, confiance={confidence:.2f}")
            
            return transcription, confidence
            
        except Exception as e:
            error_msg = f"Erreur: {str(e)}"
            event_bus.emit('stt.error', {
                'timestamp': time.time(),
                'error': error_msg
            })
            self.logger.error(f"Erreur transcription avec √©v√©nements: {e}")
            return error_msg, 0.0
    
    def prepare_for_conversation(self):
        """
        Pr√©pare l'agent pour le mode conversation en pr√©chargeant le mod√®le.
        
        Returns:
            bool: True si la pr√©paration a r√©ussi, False sinon
        """
        try:
            self.logger.info("Pr√©paration du mode conversation...")
            self._conversation_mode = True
            
            # Pr√©charger le mod√®le
            if not self._ensure_model_loaded():
                return False
            
            # Optimiser la m√©moire GPU si disponible
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.backends.cudnn.benchmark = True
            
            self.logger.info("Mode conversation pr√™t")
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la pr√©paration du mode conversation: {e}")
            self._conversation_mode = False
            return False
    
    def exit_conversation_mode(self):
        """
        Quitte le mode conversation et lib√®re les ressources r√©serv√©es.
        
        Returns:
            bool: True si la sortie a r√©ussi, False sinon
        """
        try:
            if not self._conversation_mode:
                return True
                
            self.logger.info("Sortie du mode conversation...")
            self._conversation_mode = False
            
            # Lib√©rer la m√©moire si n√©cessaire
            if self._model_loaded and not self._lazy_load:
                self._force_unload_model()
            
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la sortie du mode conversation: {e}")
            return False
    
    def record_audio_with_vad(
        self,
        max_duration: float = 10.0,
        silence_threshold: float = 0.015,
        silence_duration: float = 1.5,
        min_duration: float = 0.5
    ) -> Optional[str]:
        """
        Enregistre l'audio avec d√©tection de fin de parole (VAD).
        Arr√™te automatiquement apr√®s silence prolong√©.
        
        Args:
            max_duration: Dur√©e maximale d'enregistrement (secondes)
            silence_threshold: Seuil RMS pour d√©tecter le silence
            silence_duration: Dur√©e de silence pour arr√™ter (secondes)
            min_duration: Dur√©e minimale d'enregistrement (secondes)
            
        Returns:
            str: Chemin vers le fichier audio ou None
        """
        try:
            self.logger.info(f"Enregistrement avec VAD (max {max_duration}s)...")
            
            # Buffer pour accumuler l'audio
            audio_buffer = []
            silence_start = None
            recording_start = time.time()
            is_speaking = False
            
            # Callback pour traiter les chunks audio
            def audio_callback(indata, frames, time_info, status):
                nonlocal silence_start, is_speaking
                
                # Calculer RMS du chunk
                rms = np.sqrt(np.mean(indata**2))
                
                # Ajouter au buffer
                audio_buffer.append(indata.copy())
                
                # D√©tection parole/silence
                if rms > silence_threshold:
                    is_speaking = True
                    silence_start = None
                else:
                    # Silence d√©tect√©
                    if is_speaking and silence_start is None:
                        silence_start = time.time()
            
            # Stream audio
            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype='float32',
                callback=audio_callback,
                blocksize=int(self.sample_rate * 0.1)  # 100ms chunks
            ):
                while True:
                    elapsed = time.time() - recording_start
                    
                    # V√©rifier dur√©e max
                    if elapsed > max_duration:
                        self.logger.info(f"Dur√©e max atteinte ({max_duration}s)")
                        break
                    
                    # V√©rifier silence prolong√© (apr√®s dur√©e min)
                    if (elapsed > min_duration and 
                        silence_start and 
                        (time.time() - silence_start > silence_duration)):
                        self.logger.info(f"Fin de parole d√©tect√©e ({elapsed:.1f}s)")
                        break
                    
                    time.sleep(0.05)
            
            # Concat√©ner buffer
            if not audio_buffer:
                self.logger.error("Aucun audio captur√©")
                return None
            
            audio_data = np.concatenate(audio_buffer, axis=0)
            
            # Appliquer normalisation gain
            audio_data = audio_data * 0.3
            
            # Analyser qualit√©
            rms = np.sqrt(np.mean(audio_data**2))
            clipping = (np.abs(audio_data) > 0.99).sum()
            clipping_percent = clipping / len(audio_data) * 100
            duration = len(audio_data) / self.sample_rate
            
            self.logger.info(f"Audio captur√©: {duration:.1f}s, RMS={rms:.3f}, Clipping={clipping_percent:.1f}%")
            
            # Sauvegarder
            timestamp = int(time.time())
            audio_file = self.audio_dir / f"recording_{timestamp}.wav"
            audio_int16 = (audio_data * 32767).astype(np.int16)
            wav.write(str(audio_file), self.sample_rate, audio_int16)
            
            if audio_file.exists():
                self.logger.info(f"‚úÖ Enregistr√©: {audio_file}")
                return str(audio_file)
            else:
                self.logger.error("√âchec sauvegarde")
                return None
                
        except Exception as e:
            self.logger.error(f"Erreur enregistrement VAD: {e}")
            return None

    def record_audio(self, duration=None, max_duration=None, use_vad=False):
        """
        Enregistre l'audio depuis le microphone en utilisant AudioManager.
        
        Args:
            duration (float, optional): Dur√©e fixe d'enregistrement en secondes (d√©faut: 5.0)
            max_duration (float, optional): Dur√©e maximale avec VAD (ignor√© si use_vad=False)
            use_vad (bool): Utiliser VAD pour d√©tection fin parole (d√©faut: False)
            
        Returns:
            str: Chemin vers le fichier audio enregistr√© ou None en cas d'erreur
        """
        try:
            # Importer AudioManager et VAD
            from agents.audio_manager import audio_manager
            from agents.vad_engine import create_vad
            
            # Dur√©e par d√©faut
            if duration is None:
                duration = 5.0
            
            if use_vad and max_duration:
                # Enregistrement avec VAD
                self.logger.info(f"üé§ Enregistrement avec VAD (max {max_duration}s)...")
                
                # Cr√©er VAD
                vad = create_vad(profile="normal", sample_rate=self.sample_rate)
                
                # Enregistrer avec AudioManager
                audio_data_obj = audio_manager.record(duration=max_duration)
                
                if audio_data_obj is None:
                    self.logger.error("√âchec enregistrement AudioManager")
                    return None
                
                # Appliquer VAD pour extraire parole
                audio_speech, speech_duration = vad.process_audio(
                    audio_data_obj.samples,
                    max_duration=max_duration
                )
                
                if audio_speech is None or len(audio_speech) == 0:
                    self.logger.warning("Aucune parole d√©tect√©e par VAD")
                    # Fallback: utiliser audio complet
                    audio_data = audio_data_obj.samples
                else:
                    self.logger.info(f"‚úÖ Parole extraite par VAD: {speech_duration:.2f}s")
                    audio_data = audio_speech
                
            else:
                # Enregistrement fixe avec AudioManager
                self.logger.info(f"üé§ Enregistrement fixe: {duration}s")
                
                audio_data_obj = audio_manager.record(duration=duration)
                
                if audio_data_obj is None:
                    self.logger.error("√âchec enregistrement AudioManager")
                    return None
                
                audio_data = audio_data_obj.samples
            
            # Normalisation gain (AudioManager ne le fait pas)
            audio_data = audio_data * 0.3
            
            # Pr√©traitement audio pour am√©liorer qualit√© STT
            audio_data = self._preprocess_audio(audio_data, self.sample_rate)
            
            # Sauvegarder
            timestamp = int(time.time())
            audio_file = self.audio_dir / f"recording_{timestamp}.wav"
            audio_int16 = (audio_data * 32767).astype(np.int16)
            wav.write(str(audio_file), self.sample_rate, audio_int16)
            
            if audio_file.exists():
                self.logger.info(f"‚úÖ Audio enregistr√©: {audio_file}")
                return str(audio_file)
            else:
                self.logger.error("Fichier audio non cr√©√©")
                return None
                
        except Exception as e:
            self.logger.error(f"Erreur enregistrement: {e}")
            self.logger.error(traceback.format_exc())
            return None
    
    def cleanup(self):
        """Nettoie les ressources de l'agent."""
        try:
            self.logger.info("Nettoyage des ressources de l'agent vocal...")
            
            # Sortir du mode conversation si actif
            if self._conversation_mode:
                self.exit_conversation_mode()
            
            # D√©charger le mod√®le
            if self._model_loaded:
                self._force_unload_model()
            
            # Nettoyer la m√©moire GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("Nettoyage termin√©")
            
        except Exception as e:
            self.logger.error(f"Erreur lors du nettoyage: {e}")
            self.logger.error(traceback.format_exc())
    
    def __del__(self):
        """Destructeur de l'agent."""
        try:
            self.cleanup()
        except:
            pass

# Pour les tests unitaires
if __name__ == "__main__":
    agent = Wav2VecVoiceAgent(debug=True)
    print(f"Agent initialis√© sur {agent.device}")
    
    audio_path = input("Chemin vers un fichier audio √† transcrire (ou appuyez sur Entr√©e pour enregistrer): ")
    
    if not audio_path:
        print("Enregistrement de 5 secondes...")
        audio_path = agent.record_audio(duration=5)
        if not audio_path:
            print("Erreur d'enregistrement")
            exit(1)
    
    transcription, confidence = agent.transcribe_audio(audio_path)
    print(f"Transcription: {transcription}")
    print(f"Confiance: {confidence:.3f}")
    
    agent.cleanup()