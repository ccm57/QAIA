# Plan de Migration: Phi-3-mini + Thinking Mode

**Date:** 16 D√©cembre 2025  
**Objectif:** Remplacer Llama 3.1 8B par Phi-3-mini 3.8B avec mode thinking

---

## üìã TODO LIST

### Phase 1: T√©l√©chargement et Configuration (30 min)

- [ ] **T√âL√âCHARGER PHI-3-MINI**
  - Fichier: `Phi-3-mini-4k-instruct-q4.gguf`
  - Taille: 2.3 GB
  - Source: HuggingFace
  - Destination: `/media/ccm57/SSDIA/QAIA/models/`
  - Commande: `wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf`

- [ ] **CONFIGURER PARAM√àTRES DE BASE**
  - Fichier: `config/system_config.py`
  - Modifications:
    - `model_path`: Phi-3-mini-4k-instruct-q4.gguf
    - `n_ctx`: 2048 ‚Üí 2048 (maintenu, Phi-3 natif 4K)
    - `max_tokens`: 150 ‚Üí 100 (r√©ponses concises)
    - `temperature`: 0.7 ‚Üí 0.6 (optimal Phi-3)
    - Supprimer: `rope_freq_base`, `rope_freq_scale` (sp√©cifique Llama)

- [ ] **AJOUTER CONFIG THINKING MODE**
  - Fichier: `config/system_config.py`
  - Nouveau bloc:
    ```python
    THINKING_MODE_CONFIG = {
        "enabled": False,  # Toggle par d√©faut
        "trigger_keywords": ["analyse", "explique", "pourquoi", "comment"],
        "prompt_template": "chain_of_thought",
        "max_thinking_tokens": 150,
        "show_reasoning": True
    }
    ```

---

### Phase 2: Modification Format Prompt (20 min)

- [ ] **MODIFIER agents/rag_agent.py - PROMPT FORMAT**
  - Fichier: `agents/rag_agent.py`
  - Ligne ~350-370 (fonction `process_query`)
  - **AVANT (Llama 3.1):**
    ```python
    query = f"""<|im_start|>system
    Vous √™tes QAIA, un assistant IA utile, concis et pr√©cis.<|im_end|>
    <|im_start|>user
    {user_query}<|im_end|>
    <|im_start|>assistant
    """
    ```
  - **APR√àS (Phi-3):**
    ```python
    query = f"""<|system|>
    Vous √™tes QAIA, un assistant IA utile, concis et pr√©cis.
    <|end|>
    <|user|>
    {user_query}
    <|end|>
    <|assistant|>
    """
    ```

- [ ] **MODIFIER agents/rag_agent.py - STOP TOKENS**
  - Ligne ~135 (initialisation LlamaCpp)
  - **AVANT:**
    ```python
    stop=["<|im_end|>", "<|endoftext|>", "\n\n\n"]
    ```
  - **APR√àS:**
    ```python
    stop=["<|end|>", "<|endoftext|>"]
    ```

- [ ] **MODIFIER agents/llm_agent.py - FORMAT (si utilis√©)**
  - V√©rifier si llm_agent.py construit des prompts
  - Appliquer m√™me format que rag_agent.py

---

### Phase 3: Impl√©mentation Thinking Mode (40 min)

- [ ] **CR√âER utils/thinking_mode.py**
  - Nouveau module pour gestion thinking mode
  - Classes:
    - `ThinkingModeManager`: Gestion activation/d√©sactivation
    - `ThinkingPromptBuilder`: Construction prompts CoT
    - `ReasoningParser`: Extraction du raisonnement
  - Fonctions:
    - `detect_complex_query()`: D√©tection automatique
    - `build_thinking_prompt()`: Prompt avec CoT
    - `parse_reasoning()`: Extraction √©tapes

- [ ] **INT√âGRER THINKING MODE DANS RAG_AGENT**
  - Fichier: `agents/rag_agent.py`
  - Ajouter import: `from utils.thinking_mode import ThinkingModeManager`
  - Modifier `process_query()`:
    - D√©tecter si thinking mode requis
    - Adapter prompt selon mode
    - Parser r√©ponse pour extraire raisonnement

- [ ] **AJOUTER TOGGLE DANS INTERFACE**
  - Fichier: `interface/qaia_interface.py`
  - Ajouter checkbox "üß† Mode R√©flexion"
  - Position: √Ä c√¥t√© du mode conversation
  - Event handler: Active/d√©sactive thinking mode
  - Indicateur visuel quand actif

- [ ] **AFFICHER RAISONNEMENT DANS UI**
  - Modifier zone texte pour afficher:
    - Raisonnement (si thinking mode)
    - S√©parateur visuel
    - R√©ponse finale
  - Format:
    ```
    üß† Raisonnement:
    1. [√©tape 1]
    2. [√©tape 2]
    ---
    üí¨ R√©ponse: [r√©ponse finale]
    ```

---

### Phase 4: Nettoyage et Tests (30 min)

- [ ] **NETTOYER CACHE PYTHON**
  - Commande: `find . -name "*.pyc" -delete`
  - Commande: `find . -name "__pycache__" -type d -exec rm -rf {} +`

- [ ] **TEST 1: Mode Normal - Question Simple**
  - Lancer: `python3 launcher.py`
  - Question: "Bonjour, comment vas-tu?"
  - V√©rifier: R√©ponse rapide et concise
  - Mesurer: Temps de r√©ponse
  - Attendu: ~20-25s (vs 50s avant)

- [ ] **TEST 2: Mode Normal - Question Complexe**
  - Question: "Explique-moi la diff√©rence entre Python et Java"
  - V√©rifier: R√©ponse structur√©e
  - Attendu: ~25-30s

- [ ] **TEST 3: Thinking Mode - Question Math√©matique**
  - Activer: Mode r√©flexion
  - Question: "Si j'ai 15 pommes et j'en donne 1/3 √† Pierre, combien m'en reste-t-il?"
  - V√©rifier: 
    - Affichage du raisonnement √©tape par √©tape
    - R√©ponse correcte (10 pommes)
  - Attendu: ~30-40s (plus lent car raisonnement)

- [ ] **TEST 4: Thinking Mode - Question Logique**
  - Question: "Pourquoi le ciel est-il bleu?"
  - V√©rifier: Raisonnement scientifique visible
  - Attendu: Explication √©tape par √©tape

- [ ] **TEST 5: Conversation Multi-tours**
  - Test: 3 questions cons√©cutives
  - V√©rifier: Pas de blocage
  - V√©rifier: Contexte maintenu

- [ ] **TEST 6: RAG avec Documents**
  - Ajouter doc: `data/documents/test_phi3.txt`
  - Question: Sur contenu du document
  - V√©rifier: R√©cup√©ration contexte fonctionne

---

### Phase 5: Benchmarking et Documentation (30 min)

- [ ] **BENCHMARK LATENCE**
  - Script: `scripts/benchmark_pipeline.py`
  - Mesures:
    - Temps STT (devrait rester ~3-5s)
    - Temps LLM Phi-3 (attendu ~20-25s)
    - Temps TTS (devrait rester ~1-2s)
    - Total (attendu ~25-32s vs 50s avant)
  - Sauvegarder: `logs/performance/phi3_benchmark.json`

- [ ] **COMPARER QUALIT√â**
  - 10 questions test
  - Comparer r√©ponses Llama 3.1 vs Phi-3
  - Noter:
    - Pr√©cision
    - Pertinence
    - Style
    - Longueur

- [ ] **METTRE √Ä JOUR CHANGELOG.md**
  - Ajouter section v1.0.2
  - Documenter:
    - Migration Phi-3-mini
    - Ajout thinking mode
    - Gains de performance
    - Breaking changes (si applicable)

- [ ] **METTRE √Ä JOUR README.md**
  - Section "Mod√®les":
    - Remplacer Llama 3.1 par Phi-3-mini
    - Ajouter specs Phi-3
  - Section "Fonctionnalit√©s":
    - Ajouter mode thinking
  - Section "Performance":
    - Mettre √† jour benchmarks

---

## üîß FICHIERS √Ä MODIFIER (R√©sum√©)

| Fichier | Action | Priorit√© |
|---------|--------|----------|
| `models/` | T√©l√©charger Phi-3 | üî¥ Critique |
| `config/system_config.py` | Config LLM + thinking | üî¥ Critique |
| `agents/rag_agent.py` | Format prompt + stop | üî¥ Critique |
| `agents/llm_agent.py` | Format prompt | üü° Important |
| `utils/thinking_mode.py` | Nouveau module | üü° Important |
| `interface/qaia_interface.py` | Toggle + UI | üü° Important |
| `README.md` | Documentation | üü¢ Optionnel |
| `CHANGELOG.md` | Historique | üü¢ Optionnel |

---

## ‚ö†Ô∏è POINTS D'ATTENTION

### Diff√©rences Critiques Llama ‚Üí Phi-3

1. **Format Prompt:**
   - Llama: `<|im_start|>...<|im_end|>`
   - Phi-3: `<|system|>...<|end|>`
   - ‚ùå Ne PAS m√©langer les formats!

2. **Stop Tokens:**
   - Llama: `<|im_end|>`
   - Phi-3: `<|end|>`
   - Important pour arr√™t g√©n√©ration

3. **Context Window:**
   - Llama: 128K natif
   - Phi-3: 4K natif
   - ‚úÖ 2048 tokens OK pour les deux

4. **Temperature:**
   - Llama optimal: 0.7
   - Phi-3 optimal: 0.5-0.6
   - Ajuster pour meilleure qualit√©

5. **Thinking Mode:**
   - Nouveau concept
   - Augmente latence (+30-50%)
   - Optionnel, √† activer manuellement

---

## üìä R√âSULTATS ATTENDUS

### Performance

| M√©trique | Avant (Llama 8B) | Apr√®s (Phi-3 3.8B) | Gain |
|----------|------------------|-------------------|------|
| Latence LLM | 48s | 20-25s | -50% |
| Latence totale | 50-55s | 25-32s | -45% |
| Qualit√© | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | -10% |
| Contexte max | 128K | 4K | -97% |

### Fonctionnalit√©s

- ‚úÖ Mode normal: Conversation rapide
- ‚úÖ Thinking mode: Raisonnement visible
- ‚úÖ Compatibilit√© RAG: 100%
- ‚úÖ Interface: Inchang√©e (+ toggle)

---

## üöÄ TEMPS ESTIM√â TOTAL

- Phase 1: T√©l√©chargement + Config = **30 min**
- Phase 2: Format prompt = **20 min**
- Phase 3: Thinking mode = **40 min**
- Phase 4: Tests = **30 min**
- Phase 5: Documentation = **30 min**

**TOTAL: ~2h30**

---

## ‚úÖ CRIT√àRES DE SUCC√àS

- [ ] Phi-3 g√©n√®re des r√©ponses coh√©rentes
- [ ] Latence r√©duite de ~45%
- [ ] Thinking mode fonctionne
- [ ] Pas de blocage √† la 2√®me question
- [ ] RAG fonctionne avec Phi-3
- [ ] Tests passent tous
- [ ] Documentation √† jour

---

**Pr√™t √† commencer!** üöÄ

