# Changelog QAIA

## [2.2.8] - 2 F√©vrier 2026 - Phase 3 : ex√©cution r√©elle des commandes

### Ex√©cution r√©elle (Phase 3)
- **qaia_core.py** : `_register_command_actions()` enregistre les callbacks pour les commandes autoris√©es : arr√™te lecture (stop_speech), arr√™te enregistrement/micro (√©v√©nement `command.stop_recording`), lance/ouvre navigateur (webbrowser), active micro (message informatif).
- **interface/qaia_interface.py** : abonnement √† `command.stop_recording` ; `_on_command_stop_recording` appelle `_stop_ptt_recording(finalize=False)` pour arr√™ter le PTT sans lancer la transcription.
- **docs/PIPELINE_COMMANDES.md** : section ¬´ Actions r√©elles (Phase 3) ¬ª avec le tableau des paires et actions.

## [2.2.7] - 2 F√©vrier 2026 - Corrections tests pipeline commandes + robustesse guard

### Corrections
- **tests/test_command_pipeline.py** : appel direct √† `evaluate_command` au lieu de `self._evaluate` pour √©viter le binding (premier argument = TestCase) ; import au niveau module.
- **utils/command_guard.py** : normalisation des types pour `command_verb` et `command_target` avec `isinstance(..., str)` avant `.strip()`, √©vitant un crash en cas d‚Äôappel erron√©.

## [2.2.8] - 27 Janvier 2026 - Audit interaction vocale + corrections compl√©mentaires

### Corrections
- **agents/llm_agent.py** : initialisation d√©fensive de `max_tokens` dans `chat()` et `chat_stream()` avec `(MODEL_CONFIG.get("llm") or {}).get("max_tokens", 512)` puis coercition en `int`.
- **utils/text_processor.py** : `_remove_duplicate_consecutive_sentences()` pour supprimer phrases cons√©cutives quasi-dupliqu√©es (r√©ponses garbled) ; appel dans `process_streamed_text()`.

### Documentation
- **docs/AUDIT_INTERACTION_VOCALE.md** : audit flux STT ‚Üí LLM ‚Üí affichage ‚Üí TTS, bugs, corrections et plan de v√©rification.

## [2.2.6] - 27 Janvier 2026 - Corrections doublon pr√©fixe, NoneType*int, d√©calage TTS

### üêõ Corrections
- **Erreur `NoneType * int`** : Dans `agents/llm_agent.py`, `chat()` utilisait `max_tokens` sans l‚Äôinitialiser quand il √©tait `None` (appel depuis `dialogue_manager` sans `max_tokens`). Initialisation de `max_tokens` depuis `MODEL_CONFIG["llm"]["max_tokens"]` avant la g√©n√©ration.
- **Doublon pr√©fixe ¬´ (HH:MM) QAIA: ¬ª** :  
  - Dans `interface/components/streaming_text.py`, `replace_current_message()` supprime d√©sormais tout pr√©fixe ¬´ (HH:MM) QAIA: ¬ª du texte ins√©r√© pour √©viter le doublon dans la bulle.  
  - En cas d‚Äôerreur LLM pendant le streaming, `_on_llm_error()` remplace le contenu du message en cours au lieu d‚Äôajouter un second bloc, √©vitant ¬´ (16:06) QAIA: (16:06) QAIA: Erreur‚Ä¶ ¬ª.
- **D√©calage TTS** : Dans `_on_llm_complete()`, le TTS est lanc√© imm√©diatement apr√®s le nettoyage du texte stream√© (avant la mise √† jour UI), pour r√©duire le d√©lai entre l‚Äôaffichage du texte et le d√©but de la voix.

## [2.2.5] - 2 F√©vrier 2026 - Documentation modules + pipeline commandes

### üìö Documentation
- **docs/MODULES_FUTURS.md** : mise √† jour du statut des modules `audio_manager`, `context_manager`, `intent_detector` (d√©sormais int√©gr√©s au flux principal). Pr√©cision du r√¥le d'IntentDetector et de l'intention COMMAND pour le pipeline commandes.
- **docs/AI_INTEGRATION.md** : ajout d'IntentDetector dans la liste des agents et sch√©ma du flux (Interface ‚Üí QAIACore ‚Üí DialogueManager + IntentDetector ; COMMAND ‚Üí CommandGuard + CommandExecutor).
- **docs/PIPELINE_DESKTOP_WEB.md** : section D√©tection d'intentions (Desktop) ; mention du champ `intent` et des r√©ponses commandes dans le mode Web.

### üîß Pipeline commandes syst√®me (d√©tection + s√©curit√© + ex√©cution)
- **agents/intent_detector.py** : IntentResult enrichi (command_verb, command_target, command_subtype) ; m√©thode `parse_command()` pour extraire verbe/cible ; d√©tection COMMAND avec remplissage des champs.
- **utils/command_guard.py** : nouveau module de s√©curit√© (liste blanche verbe/cible, niveau de risque, require_confirmation, journalisation).
- **core/command_executor.py** : nouveau module d'ex√©cution contr√¥l√©e (mapping verbe/cible ‚Üí actions internes, pas de shell, timeout).
- **core/dialogue_manager.py** : branche COMMAND (guard ‚Üí confirmation si besoin ‚Üí executor) ; r√©ponses structur√©es (command_confirmation_pending, command_executed, command_refused).
- **Interface desktop** : gestion des confirmations pour les commandes √† risque.
- **Tests** : tests unitaires et d'int√©gration du pipeline (tests/test_command_pipeline.py, tests test_conversation_flow mis √† jour).

## [2.2.4] - 27 Janvier 2026 - Configuration Chroma/RAG unifi√©e

### üìö Documentation et configuration unifi√©e
- **docs/CONFIG_CHROMA_RAG.md** : documentation des variables unifi√©es (`QAIA_DATA_DIR`, `QAIA_VECTOR_DB_DIR`), mode Chroma embarqu√© uniquement, et mode fallback (RAG d√©sactiv√© sans crash).
- **README.md** : ajout d‚Äôune section ¬´ RAG et base vectorielle (Chroma) ¬ª avec lien vers la doc et mention du fallback automatique.

### üîß DevOps-Center (alignement)
- **ConfigMap Minikube** : inchang√© et conforme (pas de `CHROMA_*` utilis√©es par l‚Äôapp).
- **sync_qaia_source.sh** : commentaire rappelant d‚Äôex√©cuter le script apr√®s modification de `qaia_core.py` / `rag_agent.py` ; sync ex√©cut√© pour propager la version o√π Chroma n‚Äôest initialis√© que dans `rag_agent`.
- **Manifests secrets** : nomenclature align√©e sur `CHROMA_HOST` / `CHROMA_PORT` (env) et cl√©s secret `chroma_host` / `chroma_port` ; documentation dans README-SECRETS et qaia-configmap.yaml pr√©cisant que ces variables ne sont pas encore utilis√©es par QAIA (Chroma embarqu√© uniquement).
- **Health / UI** : v√©rification que `/health` et l‚Äôinterface exposent `vector_db: true/false` pour refl√©ter le mode RAG actif ou d√©sactiv√© (fallback).

## [2.2.3] - 2 F√©vrier 2026 - Suppression V-JEPA 2

### üóë Suppression agent V-JEPA 2 (vjepa2)
- Retrait de toute r√©f√©rence √† l'agent vjepa2 dans le projet QAIA.
- `utils/agent_manager.py` : aucun agent vjepa/vjepa2 (d√©j√† absent).
- `utils/monitoring.py` : retrait de l'agent ¬´ Vision ¬ª (affichage UI li√© √† vjepa) de la liste des agents connus.
- `docs/FICHIERS_OBSOLETES.md` : mise √† jour des r√©f√©rences √† `agents/vjepa2_agent.py` (fichier obsol√®te supprim√©).
- Suppression du checkpoint partiel vjepa2 : `models/torch_cache/hub/checkpoints/vjepa2-ac-vitg.pt.*.partial`.
- Note : les modules VJEPA2 dans `.venv` (transformers) ne sont pas modifi√©s (d√©pendance tierce).

## [2.2.2] - 31 Janvier 2026 - Priorit√© 2 (consolidations)

### üìä Monitoring unifi√©
- `utils.monitoring` devient le point d'entr√©e centralis√© (d√©l√©gation vers `MetricsCollector`).
- `utils.performance_metrics` simplifi√© en wrapper de compatibilit√©.
- Import unifi√© dans `interface/qaia_interface.py`.

### üßº Nettoyage texte centralis√©
- `utils.encoding_utils.clean_text()` d√©l√®gue au module `utils.text_processor`.
- Nettoyage des artefacts Phi-3 centralis√© dans `agents/rag_agent.py`.

### üßæ Logs & archivage
- Archivage automatique des logs de performance JSON (dossier `logs/archive/performance`).
- Variable `QAIA_LOG_ARCHIVE_DAYS` support√©e (par d√©faut 30 jours).
- `.gitignore` mis √† jour pour ignorer les logs JSON.

### üîß CI/CD
- Workflow CI simplifi√©: utilisation exclusive de `requirements.txt` (suppression de `requirements-lock.txt`).

### üß™ Tests
- Test d'int√©gration contexte/intention ajust√© pour valider `dialogue_manager.py`.

### üê≥ Docker
- Ajout d'un `Dockerfile` CPU-only (Python 3.12 slim) pour ex√©cution de `launcher.py`.
- Ajout d'un `.dockerignore` pour exclure `.venv`, logs, caches et mod√®les volumineux.

## [2.2.1] - 31 Janvier 2026 - Corrections prioritaires

### üîê S√©curit√© & configuration
- Migration de la cl√© de s√©curit√© vers `.env` (variable `QAIA_SECURITY_KEY`).
- Suppression de `config/.security_key` (cl√© d√©plac√©e hors d√©p√¥t).
- Chargement local de `.env` dans `config/system_config.py` (sans d√©pendance externe).
- Ajout de `.env`/`.env.*` dans `.gitignore`.

### üß≠ Coh√©rence des chemins
- Alignement de `QAIA_VECTOR_DB_DIR` sur `DATA_DIR` dans `launcher.py`.
- Lecture des overrides `QAIA_*_DIR` dans `config/system_config.py`.

### üß† Persistance ChromaDB
- Passage √† `chromadb.PersistentClient` dans `qaia_core.py`.

### üßπ Nettoyage
- Suppression du fichier orphelin `=0.8.0`.

## [2.2.0] - 18 D√©cembre 2025 - Interface V2 unique

### üßπ D√©commission de l‚Äôancienne interface
- Suppression de l‚Äôancienne interface graphique `agents/interface_agent.py`.
- `launcher.py` utilise d√©sormais exclusivement `interface/qaia_interface.QAIAInterface` (V2).
- `INTERFACE_MODE` (voir `config/system_config.py`) ne supporte plus `legacy` et pointe toujours vers la V2.
- Documentation mise √† jour¬†:
  - `ARBORESCENCE.txt` sans r√©f√©rence √† `interface_agent.py`,
  - `README.md` (structure projet, docs V2),
  - `docs/AI_INTEGRATION.md`,
  - `docs/decommission_old_interface.md`.

### ‚úÖ Validation & campagne de tests
- Ajout de `docs/TEST_CAMPAIGN_V2.md` (matrice de tests + proc√©dure de campagne).
- Alignement avec `docs/INTERFACE_V2_VALIDATION.md` pour la validation fonctionnelle et UX de la V2.

## [2.1.0] - 16 D√©cembre 2025 - Migration Phi-3

### üöÄ Migration Majeure
- **LLM**: Migration de Llama 3.1 8B vers Phi-3-mini-4k-instruct (3.8B)
  - Latence moyenne: -45% (46s ‚Üí 25.5s)
  - Questions cons√©cutives: -59% (46s ‚Üí 19s)
  - RAM utilis√©e: -58% (5.5GB ‚Üí 2.3GB)
  - Format prompt: `<|system|>` / `<|user|>` / `<|assistant|>` (Phi-3)
  - Stop tokens: `<|end|>` au lieu de `<|im_end|>`

### ‚öôÔ∏è Configuration
- Ajout ressources d√©di√©es Phi-3 (RAM limit 12GB, CPU threads 6)
- Param√®tres optimis√©s: temp=0.6, max_tokens=100
- Suppression param√®tres Llama (rope_freq_base, rope_freq_scale)

### üéØ Prompt Syst√®me
Refonte compl√®te avec directives utilisateur:
- Principe de v√©rit√© absolue
- Citation de sources obligatoire
- Protection et s√©curit√© prioritaires
- Pr√©sentation: "Bonjour, je suis QAIA votre assistante multimodale..."

### üìù Fichiers Modifi√©s
1. **config/system_config.py**: Nouveau mod√®le, ressources, prompt syst√®me
2. **agents/llm_agent.py**: Format prompt Phi-3, nettoyage artefacts
3. **agents/rag_agent.py**: Stop tokens Phi-3
4. **qaia_core.py**: Format prompt fallback Phi-3
5. **README.md**: Documentation mise √† jour
6. **launch_qaia.sh**: Message d'accueil Phi-3

### üßπ Nettoyage
- Suppression scripts de test obsol√®tes
- Nettoyage r√©f√©rences Llama dans code/docs
- Cache Python nettoy√©

## [1.0.1] - 16 D√©cembre 2025

### ‚úÖ Corrections Critiques

#### Blocage Audio (2√®me question)
**Probl√®me:** Le syst√®me se bloquait syst√©matiquement √† la 2√®me question en mode vocal
**Cause:** Flag `ptt_stopping` non r√©initialis√© apr√®s traitement
**Solution:** Ajout `self.ptt_stopping = False` dans `_process_text_thread` finally block
**Fichier:** `interface/qaia_interface.py` ligne 509

#### Saturation Audio
**Probl√®me:** Audio satur√© (26.6% clipping, RMS 18,496) ‚Üí transcription impossible
**Causes:**
- Volume microphone trop √©lev√© (80%)
- Pas de r√©duction de gain dans le code
**Solutions:**
- Volume micro r√©duit √† 30%
- Gain audio r√©duit √† 0.3 (-10dB) dans `interface/qaia_interface.py` ligne 618
**Impact:** RMS attendu < 10,000, clipping < 5%

### üìù Fichiers Modifi√©s

1. **interface/qaia_interface.py**
   - Ligne 509: Ajout r√©initialisation `ptt_stopping`
   - Ligne 618: Ajout gain audio 0.3

### üÜï Nouveaux Outils

1. **scripts/test_audio_pipeline.py**
   - Diagnostic complet pipeline audio
   - Analyse qualit√© (RMS, clipping, silence)
   - Test pr√©traitement et transcription

2. **test_qaia.sh**
   - Script de test rapide
   - Configuration automatique volume micro
   - Instructions interactives

### üóëÔ∏è Nettoyage Arborescence

**Supprim√©s:**
- 47 fichiers de rapport/audit redondants (15 d√©c)
- 7 fichiers .txt obsol√®tes
- 7 fichiers temporaires racine

**R√©sultat:** docs/ : 60 ‚Üí 4 fichiers essentiels

---

## [1.0.0] - 15 D√©cembre 2025

### üéâ Version Initiale

#### Fonctionnalit√©s
- Reconnaissance vocale (Wav2Vec2 fran√ßais)
- Synth√®se vocale f√©minine (Piper TTS)
- RAG avec ChromaDB
- LLM Llama 3.1 8B (Q4_K_M)
- Interface graphique Tkinter
- Mode conversation Push-to-Talk

#### Architecture
- Agents modulaires (voice, speech, RAG, LLM, vision)
- Syst√®me de logs centralis√©
- Base de donn√©es SQLite
- Pr√©traitement audio (filtrage, normalisation)

---

## Configuration Syst√®me Actuelle

**Mat√©riel:** Intel i7-7700HQ, 40GB RAM, CPU only
**OS:** Linux 6.14.0
**Python:** 3.11
**Mod√®les:**
- LLM: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
- STT: jonatasgrosman/wav2vec2-large-xlsr-53-french
- TTS: Piper fr_FR-siwis-medium (voix f√©minine)

**Param√®tres Optimis√©s:**
- LLM: n_ctx=2048, max_tokens=150, n_batch=512
- Audio: sample_rate=16kHz, gain=0.3, volume_micro=30%
- TTS: volume=0.3 (30%)

